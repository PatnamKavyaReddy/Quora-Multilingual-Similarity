{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5130fd2cb5</td>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b72532a0b</td>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3931fbe82a</td>\n",
       "      <td>Des petites choses comme celles-là font une di...</td>\n",
       "      <td>J'essayais d'accomplir quelque chose.</td>\n",
       "      <td>fr</td>\n",
       "      <td>French</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5622f0c60b</td>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86aaa48b45</td>\n",
       "      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n",
       "      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n",
       "      <td>th</td>\n",
       "      <td>Thai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>2b78e2a914</td>\n",
       "      <td>The results of even the most well designed epi...</td>\n",
       "      <td>All studies have the same amount of uncertaint...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>7e9943d152</td>\n",
       "      <td>But there are two kinds of  the pleasure of do...</td>\n",
       "      <td>But there are two kinds of the pleasure of doi...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>5085923e6c</td>\n",
       "      <td>The important thing is to realize that it's wa...</td>\n",
       "      <td>It cannot be moved, now or ever.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>fc8e2fd1fe</td>\n",
       "      <td>At the west end is a detailed model of the who...</td>\n",
       "      <td>The model temple complex is at the east end.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>44301dfb14</td>\n",
       "      <td>For himself he chose Atat??rk, or Father of th...</td>\n",
       "      <td>Ataturk was the father of the Turkish nation.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12120 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                            premise  \\\n",
       "0      5130fd2cb5  and these comments were considered in formulat...   \n",
       "1      5b72532a0b  These are issues that we wrestle with in pract...   \n",
       "2      3931fbe82a  Des petites choses comme celles-là font une di...   \n",
       "3      5622f0c60b  you know they can't really defend themselves l...   \n",
       "4      86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
       "...           ...                                                ...   \n",
       "12115  2b78e2a914  The results of even the most well designed epi...   \n",
       "12116  7e9943d152  But there are two kinds of  the pleasure of do...   \n",
       "12117  5085923e6c  The important thing is to realize that it's wa...   \n",
       "12118  fc8e2fd1fe  At the west end is a detailed model of the who...   \n",
       "12119  44301dfb14  For himself he chose Atat??rk, or Father of th...   \n",
       "\n",
       "                                              hypothesis lang_abv language  \\\n",
       "0      The rules developed in the interim were put to...       en  English   \n",
       "1      Practice groups are not permitted to work on t...       en  English   \n",
       "2                  J'essayais d'accomplir quelque chose.       fr   French   \n",
       "3      They can't defend themselves because of their ...       en  English   \n",
       "4        เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai   \n",
       "...                                                  ...      ...      ...   \n",
       "12115  All studies have the same amount of uncertaint...       en  English   \n",
       "12116  But there are two kinds of the pleasure of doi...       en  English   \n",
       "12117                   It cannot be moved, now or ever.       en  English   \n",
       "12118       The model temple complex is at the east end.       en  English   \n",
       "12119      Ataturk was the father of the Turkish nation.       en  English   \n",
       "\n",
       "       label  \n",
       "0          0  \n",
       "1          2  \n",
       "2          0  \n",
       "3          0  \n",
       "4          1  \n",
       "...      ...  \n",
       "12115      2  \n",
       "12116      0  \n",
       "12117      2  \n",
       "12118      2  \n",
       "12119      0  \n",
       "\n",
       "[12120 rows x 6 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = df[df['language'] == 'English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5130fd2cb5</td>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b72532a0b</td>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5622f0c60b</td>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fdcd1bd867</td>\n",
       "      <td>From Cockpit Country to St. Ann's Bay</td>\n",
       "      <td>From St. Ann's Bay to Cockpit Country.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7cfb3d272c</td>\n",
       "      <td>Look, it's your skin, but you're going to be i...</td>\n",
       "      <td>The boss will fire you if he sees you slacking...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>2b78e2a914</td>\n",
       "      <td>The results of even the most well designed epi...</td>\n",
       "      <td>All studies have the same amount of uncertaint...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>7e9943d152</td>\n",
       "      <td>But there are two kinds of  the pleasure of do...</td>\n",
       "      <td>But there are two kinds of the pleasure of doi...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>5085923e6c</td>\n",
       "      <td>The important thing is to realize that it's wa...</td>\n",
       "      <td>It cannot be moved, now or ever.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>fc8e2fd1fe</td>\n",
       "      <td>At the west end is a detailed model of the who...</td>\n",
       "      <td>The model temple complex is at the east end.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>44301dfb14</td>\n",
       "      <td>For himself he chose Atat??rk, or Father of th...</td>\n",
       "      <td>Ataturk was the father of the Turkish nation.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6870 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                            premise  \\\n",
       "0      5130fd2cb5  and these comments were considered in formulat...   \n",
       "1      5b72532a0b  These are issues that we wrestle with in pract...   \n",
       "3      5622f0c60b  you know they can't really defend themselves l...   \n",
       "7      fdcd1bd867              From Cockpit Country to St. Ann's Bay   \n",
       "8      7cfb3d272c  Look, it's your skin, but you're going to be i...   \n",
       "...           ...                                                ...   \n",
       "12115  2b78e2a914  The results of even the most well designed epi...   \n",
       "12116  7e9943d152  But there are two kinds of  the pleasure of do...   \n",
       "12117  5085923e6c  The important thing is to realize that it's wa...   \n",
       "12118  fc8e2fd1fe  At the west end is a detailed model of the who...   \n",
       "12119  44301dfb14  For himself he chose Atat??rk, or Father of th...   \n",
       "\n",
       "                                              hypothesis lang_abv language  \\\n",
       "0      The rules developed in the interim were put to...       en  English   \n",
       "1      Practice groups are not permitted to work on t...       en  English   \n",
       "3      They can't defend themselves because of their ...       en  English   \n",
       "7                 From St. Ann's Bay to Cockpit Country.       en  English   \n",
       "8      The boss will fire you if he sees you slacking...       en  English   \n",
       "...                                                  ...      ...      ...   \n",
       "12115  All studies have the same amount of uncertaint...       en  English   \n",
       "12116  But there are two kinds of the pleasure of doi...       en  English   \n",
       "12117                   It cannot be moved, now or ever.       en  English   \n",
       "12118       The model temple complex is at the east end.       en  English   \n",
       "12119      Ataturk was the father of the Turkish nation.       en  English   \n",
       "\n",
       "       label  \n",
       "0          0  \n",
       "1          2  \n",
       "3          0  \n",
       "7          2  \n",
       "8          1  \n",
       "...      ...  \n",
       "12115      2  \n",
       "12116      0  \n",
       "12117      2  \n",
       "12118      2  \n",
       "12119      0  \n",
       "\n",
       "[6870 rows x 6 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english #0, 1, 2 corresponds to entailment, neutral, and contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning each sentence.\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "#will replace the html characters with \" \"\n",
    "    text=re.sub('<.*?>', ' ', text)  \n",
    "    #To remove the punctuations\n",
    "    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n",
    "    #will consider only alphabets and numerics\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)  \n",
    "    #will replace newline with space\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    #will convert to lower case\n",
    "    text = text.lower()\n",
    "    # will split and join the words\n",
    "    text=' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-124-f0279ad290d7>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english['premise_c'] = [clean_text(sentence) for sentence in df_english['premise']]\n",
      "<ipython-input-124-f0279ad290d7>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english['hypothesis_c'] = [clean_text(sentence) for sentence in df_english['hypothesis']]\n"
     ]
    }
   ],
   "source": [
    "df_english['premise_c'] = [clean_text(sentence) for sentence in df_english['premise']] \n",
    "df_english['hypothesis_c'] = [clean_text(sentence) for sentence in df_english['hypothesis']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look, it's your skin, but you're going to be in trouble if you don't get busy.\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english['premise'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'look its your skin but youre going to be in trouble if you dont get busy'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english['premise_c'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = df_english.drop(columns=['premise', 'hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removal of stopwords\n",
    "\n",
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# df_english['premise_stop'] = [remove_stopwords(sentence) for sentence in df_english['premise_c']]\n",
    "# df_english['hypo_stop'] = [remove_stopwords(sentence) for sentence in df_english['hypothesis_c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_english['premise_c'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_english['premise_stop'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [and, these, comments, were, considered, in, f...\n",
      "1     [these, are, issues, that, we, wrestle, with, ...\n",
      "3     [you, know, they, cant, really, defend, themse...\n",
      "7           [from, cockpit, country, to, st, anns, bay]\n",
      "8     [look, its, your, skin, but, youre, going, to,...\n",
      "16    [if, you, people, only, knew, how, fatally, ea...\n",
      "17    [my, own, little, corner, of, the, world, poli...\n",
      "18    [life, in, prison, then, hes, available, for, ...\n",
      "19    [the, streets, are, crammed, with, vendors, se...\n",
      "20    [north, of, mytilini, stop, at, the, village, ...\n",
      "Name: tokenized_premise, dtype: object\n",
      "\n",
      "\n",
      "0     [the, rules, developed, in, the, interim, were...\n",
      "1     [practice, groups, are, not, permitted, to, wo...\n",
      "3     [they, cant, defend, themselves, because, of, ...\n",
      "7           [from, st, anns, bay, to, cockpit, country]\n",
      "8     [the, boss, will, fire, you, if, he, sees, you...\n",
      "16    [many, people, have, poisoned, someone, by, mi...\n",
      "17                   [an, example, is, policy, wonking]\n",
      "18    [the, system, is, corrupt, because, he, wont, ...\n",
      "19    [vendors, have, lined, the, streets, with, tor...\n",
      "20    [there, is, nothing, special, to, see, in, the...\n",
      "Name: tokenized_hypo, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_english['tokenized_premise'] = [simple_preprocess(sentence, deacc=True) for sentence in df_english['premise_c']] \n",
    "df_english['tokenized_hypo'] = [simple_preprocess(sentence, deacc=True) for sentence in df_english['hypothesis_c']] \n",
    "print(df_english['tokenized_premise'].head(10))\n",
    "print(\"\\n\")\n",
    "print(df_english['tokenized_hypo'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [and, these, comment, were, consid, in, formul...\n",
      "1     [these, ar, issu, that, we, wrestl, with, in, ...\n",
      "3     [you, know, thei, cant, realli, defend, themse...\n",
      "7            [from, cockpit, countri, to, st, ann, bai]\n",
      "8     [look, it, your, skin, but, your, go, to, be, ...\n",
      "16    [if, you, peopl, onli, knew, how, fatal, easi,...\n",
      "17    [my, own, littl, corner, of, the, world, polic...\n",
      "18    [life, in, prison, then, he, avail, for, parol...\n",
      "19    [the, street, ar, cram, with, vendor, sell, sh...\n",
      "20    [north, of, mytilini, stop, at, the, villag, o...\n",
      "Name: stemmed_premise, dtype: object\n",
      "0     [the, rule, develop, in, the, interim, were, p...\n",
      "1     [practic, group, ar, not, permit, to, work, on...\n",
      "3     [thei, cant, defend, themselv, becaus, of, the...\n",
      "7            [from, st, ann, bai, to, cockpit, countri]\n",
      "8     [the, boss, will, fire, you, if, he, see, you,...\n",
      "16      [mani, peopl, have, poison, someon, by, mistak]\n",
      "17                       [an, exampl, is, polici, wonk]\n",
      "18    [the, system, is, corrupt, becaus, he, wont, b...\n",
      "19    [vendor, have, line, the, street, with, torch,...\n",
      "20    [there, is, noth, special, to, see, in, the, v...\n",
      "Name: stemmed_hypo, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "df_english['stemmed_premise'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df_english['tokenized_premise']]\n",
    "print(df_english['stemmed_premise'].head(10))\n",
    "df_english['stemmed_hypo'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df_english['tokenized_hypo']]\n",
    "print(df_english['stemmed_hypo'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_english[['stemmed_premise', 'stemmed_hypo']], \n",
    "                                                    df_english['label'], \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "Y_train = Y_train.to_frame()\n",
    "Y_train = Y_train.reset_index()\n",
    "Y_test = Y_test.to_frame()\n",
    "Y_test = Y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1938\n",
      "2    1836\n",
      "1    1722\n",
      "Name: label, dtype: int64\n",
      "0    489\n",
      "1    444\n",
      "2    441\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Y_train['label'].value_counts())\n",
    "print(Y_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "Here, we use: Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download) GloVe pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pre-trained vectors into dictionary that hold mappings between words and the embedding vectors\n",
    "import numpy as np\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(\"glove.42B.300d.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.6884e-01, -1.7578e-01,  4.5401e-02, -3.4579e-01,  8.3458e-02,\n",
       "        4.5321e-02, -3.2051e+00,  4.2430e-01, -2.9677e-01, -3.3661e-02,\n",
       "        1.3370e-01,  4.4926e-01,  1.5338e-01,  2.8428e-01, -1.1154e-01,\n",
       "        4.9325e-03,  1.8104e-01, -7.4951e-01,  3.4237e-02, -6.6673e-01,\n",
       "       -8.3367e-02,  1.2997e-01, -4.7923e-01, -1.0729e-01,  1.9265e-02,\n",
       "       -1.4062e-01,  7.9975e-02, -1.9749e-01, -1.6811e-01,  1.1539e-01,\n",
       "       -2.6112e-01,  4.5084e-01, -3.7895e-01,  3.2797e-01, -6.8369e-01,\n",
       "       -5.7115e-02, -4.5942e-01,  1.5110e-01, -3.0112e-01, -4.0784e-01,\n",
       "        3.6336e-02,  4.1883e-02,  5.1948e-01,  4.3310e-02, -4.5785e-03,\n",
       "        7.5172e-02, -6.2771e-02, -5.3660e-02,  1.0484e-01,  4.8578e-01,\n",
       "        2.3089e-01, -3.7410e-01,  1.6982e-01,  1.0525e-01, -3.8011e-02,\n",
       "        5.0624e-01,  3.2189e-01,  1.4993e-01, -3.9319e-01,  9.9445e-02,\n",
       "       -1.9707e-01,  4.0726e-01,  3.0029e-01, -2.8234e-01, -4.1426e-01,\n",
       "        5.4300e-02,  9.6829e-02,  1.1243e-01,  8.3774e-01,  3.1980e-01,\n",
       "       -8.3176e-01,  6.0566e-03,  2.2647e-01,  5.2784e-02,  1.3959e-01,\n",
       "       -6.5868e-01, -1.4722e-02,  2.7809e-01,  8.3907e-02, -8.1779e-02,\n",
       "        3.6699e-02, -1.3082e+00,  3.3592e-01, -5.5247e-01, -6.1383e-02,\n",
       "       -6.3180e-02,  3.8834e-01, -2.2143e-01,  1.7056e-01,  1.3108e-01,\n",
       "        2.1877e-01, -3.3690e-01, -1.0263e-01,  1.5078e-01,  1.6879e-02,\n",
       "        2.0190e-01, -2.5009e+00,  7.9831e-02,  2.6364e-01, -1.7007e-01,\n",
       "        4.0549e-01,  1.4269e-01,  7.4205e-02, -3.4229e-01,  2.3488e-01,\n",
       "        6.7490e-01, -1.6864e-01, -9.3316e-01, -5.0629e-01, -6.2795e-01,\n",
       "        1.2786e-01,  4.5360e-01,  1.4549e-01, -4.3255e-01,  4.1621e-01,\n",
       "       -3.5675e-01, -3.0739e-01,  8.8606e-02,  2.7011e-01, -1.0473e-01,\n",
       "       -1.3768e-01, -2.2270e-01, -3.2510e-01, -1.4889e-01, -1.3297e-01,\n",
       "        2.8255e-01, -1.8829e-01,  8.8701e-02, -1.7800e-01,  7.5877e-02,\n",
       "       -2.2150e-01, -1.8141e-01, -4.0465e-01, -1.4992e-02,  4.0371e-01,\n",
       "       -2.4641e-01, -1.5825e-02,  1.9287e-01, -1.0390e-01,  1.2438e-02,\n",
       "        2.2652e-01, -9.5868e-02,  1.2312e-01, -7.0940e-01,  2.1431e-02,\n",
       "       -9.0259e-03, -1.3761e-01, -7.4101e-02,  6.1052e-01,  1.5461e-01,\n",
       "        1.3279e-01, -8.6502e-02, -3.6166e-01,  1.5166e-01, -1.7611e-01,\n",
       "        1.0678e-01,  5.3273e-01,  4.5176e-04, -2.9820e-01, -1.0361e-01,\n",
       "       -3.5762e-01, -1.5369e-02, -1.4889e-01, -3.3557e-02, -3.5515e-01,\n",
       "        3.5471e-01, -4.3580e-01, -5.2672e-01,  3.2912e-01, -5.3358e-01,\n",
       "       -3.3422e-02,  3.1608e-01,  6.1683e-02,  7.9933e-01, -4.3619e-02,\n",
       "        6.1413e-01,  1.9004e-01, -6.8713e-02,  8.0825e-03,  2.9881e-01,\n",
       "        6.2538e-01, -1.5442e-01, -3.2420e-02, -2.3852e-01, -1.7233e-01,\n",
       "       -2.8439e-01, -2.3009e-01,  6.1503e-02,  9.7990e-02, -1.3912e-01,\n",
       "        2.4454e-01,  2.1732e-01, -3.0020e-01,  2.0857e-01,  6.5315e-02,\n",
       "       -7.2739e-02, -4.2203e-01,  4.5738e-01, -3.3834e-02, -2.9431e-01,\n",
       "       -2.9015e-01,  1.3171e-01,  1.7338e-01, -2.1529e-01, -3.3860e-02,\n",
       "        1.0072e-01,  1.3231e-01, -6.2709e-01, -2.3567e-01,  4.0249e-01,\n",
       "        2.4637e-01,  4.0739e-01,  2.1072e-01, -6.7639e-02,  3.0044e-01,\n",
       "       -3.5585e-01,  5.4233e-02, -7.8400e-02,  4.2117e-01,  2.3115e-01,\n",
       "       -1.5103e-01, -1.2511e-01,  1.7310e-01, -3.9190e-02, -2.0228e+00,\n",
       "       -3.5663e-01, -5.6551e-01,  1.3831e-01,  2.0617e-03, -4.9909e-01,\n",
       "        1.8770e-01, -2.6089e-01,  1.0783e-01, -1.1861e-01, -1.5221e-01,\n",
       "       -2.9977e-01,  1.9833e-01,  9.4002e-02, -1.8877e-01,  3.2032e-01,\n",
       "       -4.1177e-01,  4.4900e-03,  3.8603e-01, -1.5483e-01,  1.3709e-02,\n",
       "       -1.7729e-01, -3.2633e-01, -2.2133e-01,  2.4323e-01,  6.2422e-01,\n",
       "       -2.5009e-01,  6.8557e-03,  6.4180e-01,  7.2562e-02, -2.9666e-01,\n",
       "        1.3318e-01, -1.8578e-01, -2.5633e-01,  5.1510e-01,  1.1488e-01,\n",
       "        2.4487e-01, -1.6823e-01, -3.4523e-02,  2.2019e-01,  6.9400e-02,\n",
       "        2.1943e-01,  1.5072e-01, -1.9015e-01, -7.4507e-02,  4.3008e-01,\n",
       "       -3.9614e-01,  2.7464e-02,  5.0860e-01,  4.4127e-01,  1.5594e-01,\n",
       "        2.3031e-01,  2.1797e-01,  1.7829e-01, -3.9284e-01,  3.3233e-01,\n",
       "        1.7411e-01,  7.3443e-02,  1.5066e-01, -3.5184e-01,  9.0683e-02,\n",
       "        4.2152e-01,  3.9068e-02, -3.0437e-01, -1.7221e-01,  7.2297e-02,\n",
       "        1.7024e-01, -7.9436e-02,  1.1896e-01, -1.7506e-01,  4.4333e-01,\n",
       "       -2.0586e-01,  2.2518e-01,  6.1715e-02, -1.2641e-01, -2.3714e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['among']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['among'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['vector_p'] = [[] for i in range(5496)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['vector_h'] = [[] for i in range(5496)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['difference'] = [[] for i in range(5496)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>stemmed_premise</th>\n",
       "      <th>stemmed_hypo</th>\n",
       "      <th>vector_p</th>\n",
       "      <th>vector_h</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3690</td>\n",
       "      <td>[among, the, sight, in, bezier, ar, the, ancie...</td>\n",
       "      <td>[there, is, noth, interest, about, bezier, bec...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8344</td>\n",
       "      <td>[in, april, the, sultan, armi, mass, outsid, t...</td>\n",
       "      <td>[there, were, ten, time, as, mani, of, the, su...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5426</td>\n",
       "      <td>[when, the, two, nation, divid, it, up, franc,...</td>\n",
       "      <td>[franc, end, up, not, get, the, salt, pound]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6436</td>\n",
       "      <td>[over, most, of, the, and, the, us, wa, abl, t...</td>\n",
       "      <td>[the, us, could, invest, more, than, it, save,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794</td>\n",
       "      <td>[in, my, crossfir, dai, wa, patron, even, by, ...</td>\n",
       "      <td>[dure, crossfir, even, sam, donaldson, patron,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>1596</td>\n",
       "      <td>[and, and, so, you, know, like, everi, other, ...</td>\n",
       "      <td>[ticket, to, see, chima, para, diso, cost, monei]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>9183</td>\n",
       "      <td>[thi, make, it, incumb, on, the, govern, to, c...</td>\n",
       "      <td>[there, is, no, need, for, the, govern, to, cr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>7027</td>\n",
       "      <td>[the, govern, statut, provid, that, committe, ...</td>\n",
       "      <td>[no, on, can, recommend, an, individu, to, the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>415</td>\n",
       "      <td>[there, ar, two, challeng, to, these, top, dog]</td>\n",
       "      <td>[these, top, dog, face, two, tough, financi, c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>9119</td>\n",
       "      <td>[on, naxo, you, can, walk, through, the, prett...</td>\n",
       "      <td>[instead, of, walk, through, the, villag, of, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                    stemmed_premise  \\\n",
       "0      3690  [among, the, sight, in, bezier, ar, the, ancie...   \n",
       "1      8344  [in, april, the, sultan, armi, mass, outsid, t...   \n",
       "2      5426  [when, the, two, nation, divid, it, up, franc,...   \n",
       "3      6436  [over, most, of, the, and, the, us, wa, abl, t...   \n",
       "4      1794  [in, my, crossfir, dai, wa, patron, even, by, ...   \n",
       "...     ...                                                ...   \n",
       "5491   1596  [and, and, so, you, know, like, everi, other, ...   \n",
       "5492   9183  [thi, make, it, incumb, on, the, govern, to, c...   \n",
       "5493   7027  [the, govern, statut, provid, that, committe, ...   \n",
       "5494    415    [there, ar, two, challeng, to, these, top, dog]   \n",
       "5495   9119  [on, naxo, you, can, walk, through, the, prett...   \n",
       "\n",
       "                                           stemmed_hypo vector_p vector_h  \\\n",
       "0     [there, is, noth, interest, about, bezier, bec...       []       []   \n",
       "1     [there, were, ten, time, as, mani, of, the, su...       []       []   \n",
       "2          [franc, end, up, not, get, the, salt, pound]       []       []   \n",
       "3     [the, us, could, invest, more, than, it, save,...       []       []   \n",
       "4     [dure, crossfir, even, sam, donaldson, patron,...       []       []   \n",
       "...                                                 ...      ...      ...   \n",
       "5491  [ticket, to, see, chima, para, diso, cost, monei]       []       []   \n",
       "5492  [there, is, no, need, for, the, govern, to, cr...       []       []   \n",
       "5493  [no, on, can, recommend, an, individu, to, the...       []       []   \n",
       "5494  [these, top, dog, face, two, tough, financi, c...       []       []   \n",
       "5495  [instead, of, walk, through, the, villag, of, ...       []       []   \n",
       "\n",
       "     difference  \n",
       "0            []  \n",
       "1            []  \n",
       "2            []  \n",
       "3            []  \n",
       "4            []  \n",
       "...         ...  \n",
       "5491         []  \n",
       "5492         []  \n",
       "5493         []  \n",
       "5494         []  \n",
       "5495         []  \n",
       "\n",
       "[5496 rows x 6 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-138-2efa23d3deb8>:20: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  array_vector_p = np.array(array_vector_p).astype(np.float)\n",
      "<ipython-input-138-2efa23d3deb8>:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  array_vector_h = np.array(array_vector_h).astype(np.float)\n",
      "<ipython-input-138-2efa23d3deb8>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['vector_p'][index] = model_vector_p\n",
      "<ipython-input-138-2efa23d3deb8>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['vector_h'][index] = model_vector_h\n",
      "D:\\Users\\tommy\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "D:\\Users\\tommy\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-138-2efa23d3deb8>:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['vector_p'][index] = [int(0) for i in range(300)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_vector(token):\n",
    "    if token in embeddings_dict:\n",
    "        return embeddings_dict[token]\n",
    "    else:\n",
    "        return [int(0) for i in range(300)]\n",
    "\n",
    "for index, row in X_train.iterrows():\n",
    "    array_vector_p = []\n",
    "    for token in row['stemmed_premise']:\n",
    "        word_vector = get_word_vector(token)\n",
    "        array_vector_p.append(word_vector)\n",
    "        \n",
    "    array_vector_h = []\n",
    "    for token in row['stemmed_hypo']:\n",
    "        word_vector = get_word_vector(token)\n",
    "        array_vector_h.append(word_vector)\n",
    "        \n",
    "    array_vector_p = np.array(array_vector_p).astype(np.float)\n",
    "    array_vector_h = np.array(array_vector_h).astype(np.float)\n",
    "    \n",
    "#     if(index == 0):\n",
    "#         print(array_vector_p)\n",
    "    model_vector_p = (np.mean(array_vector_p, axis=0)).tolist()\n",
    "    model_vector_h = (np.mean(array_vector_h, axis=0)).tolist()\n",
    "\n",
    "    if type(model_vector_p) is list:  \n",
    "        X_train['vector_p'][index] = model_vector_p\n",
    "    else:\n",
    "        X_train['vector_p'][index] = [int(0) for i in range(300)]\n",
    "        \n",
    "    if type(model_vector_h) is list:  \n",
    "        X_train['vector_h'][index] = model_vector_h\n",
    "    else:\n",
    "        X_train['vector_h'][index] = [int(0) for i in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5496 entries, 0 to 5495\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   index            5496 non-null   int64 \n",
      " 1   stemmed_premise  5496 non-null   object\n",
      " 2   stemmed_hypo     5496 non-null   object\n",
      " 3   vector_p         5496 non-null   object\n",
      " 4   vector_h         5496 non-null   object\n",
      " 5   difference       5496 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 257.8+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-140-37257d207415>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['difference'][index] = np.array(X_train['vector_h'][index]) - np.array(X_train['vector_p'][index])\n"
     ]
    }
   ],
   "source": [
    "for index, row in X_train.iterrows():\n",
    "    X_train['difference'][index] = np.array(X_train['vector_h'][index]) - np.array(X_train['vector_p'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>stemmed_premise</th>\n",
       "      <th>stemmed_hypo</th>\n",
       "      <th>vector_p</th>\n",
       "      <th>vector_h</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3690</td>\n",
       "      <td>[among, the, sight, in, bezier, ar, the, ancie...</td>\n",
       "      <td>[there, is, noth, interest, about, bezier, bec...</td>\n",
       "      <td>[0.0042721433298928396, -0.03233098472202463, ...</td>\n",
       "      <td>[0.0435828339929382, 0.23425666491190592, -0.0...</td>\n",
       "      <td>[0.03931069066304536, 0.26658764963393056, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8344</td>\n",
       "      <td>[in, april, the, sultan, armi, mass, outsid, t...</td>\n",
       "      <td>[there, were, ten, time, as, mani, of, the, su...</td>\n",
       "      <td>[-0.0676229025957582, -0.05701556266285479, 0....</td>\n",
       "      <td>[-0.02018942977883853, 0.038256624015048146, -...</td>\n",
       "      <td>[0.047433472816919675, 0.09527218667790294, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5426</td>\n",
       "      <td>[when, the, two, nation, divid, it, up, franc,...</td>\n",
       "      <td>[franc, end, up, not, get, the, salt, pound]</td>\n",
       "      <td>[0.062390736575859286, -0.016578831140779786, ...</td>\n",
       "      <td>[-0.023860372428316623, 0.05636150110512972, 0...</td>\n",
       "      <td>[-0.0862511090041759, 0.07294033224590951, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6436</td>\n",
       "      <td>[over, most, of, the, and, the, us, wa, abl, t...</td>\n",
       "      <td>[the, us, could, invest, more, than, it, save,...</td>\n",
       "      <td>[0.06283004954457283, 0.02205984862521291, -0....</td>\n",
       "      <td>[0.03486745432019234, 0.015463362024589018, -0...</td>\n",
       "      <td>[-0.027962595224380493, -0.006596486600623891,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794</td>\n",
       "      <td>[in, my, crossfir, dai, wa, patron, even, by, ...</td>\n",
       "      <td>[dure, crossfir, even, sam, donaldson, patron,...</td>\n",
       "      <td>[0.11890019625425338, -0.02407050523906946, 0....</td>\n",
       "      <td>[-0.05265214440545866, -0.13422715025288717, -...</td>\n",
       "      <td>[-0.17155234065971203, -0.11015664501381771, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>1596</td>\n",
       "      <td>[and, and, so, you, know, like, everi, other, ...</td>\n",
       "      <td>[ticket, to, see, chima, para, diso, cost, monei]</td>\n",
       "      <td>[0.01040780481191412, -0.04228932386444461, -0...</td>\n",
       "      <td>[0.04032112471759319, -0.16984214953845367, 0....</td>\n",
       "      <td>[0.029913319905679074, -0.12755282567400905, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>9183</td>\n",
       "      <td>[thi, make, it, incumb, on, the, govern, to, c...</td>\n",
       "      <td>[there, is, no, need, for, the, govern, to, cr...</td>\n",
       "      <td>[0.04760168118001376, 0.06779161074923144, -0....</td>\n",
       "      <td>[-0.07817578880737225, 0.01920080160101255, -0...</td>\n",
       "      <td>[-0.125777469987386, -0.048590809148218894, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>7027</td>\n",
       "      <td>[the, govern, statut, provid, that, committe, ...</td>\n",
       "      <td>[no, on, can, recommend, an, individu, to, the...</td>\n",
       "      <td>[-0.05681318695270369, -0.09089089054526445, -...</td>\n",
       "      <td>[-0.012223369817630473, -0.08494136313145811, ...</td>\n",
       "      <td>[0.04458981713507321, 0.0059495274138063325, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>415</td>\n",
       "      <td>[there, ar, two, challeng, to, these, top, dog]</td>\n",
       "      <td>[these, top, dog, face, two, tough, financi, c...</td>\n",
       "      <td>[-0.014065003953874111, 0.06840262585319579, -...</td>\n",
       "      <td>[-0.07580050267279148, 0.35522675164975226, 0....</td>\n",
       "      <td>[-0.06173549871891737, 0.2868241257965565, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>9119</td>\n",
       "      <td>[on, naxo, you, can, walk, through, the, prett...</td>\n",
       "      <td>[instead, of, walk, through, the, villag, of, ...</td>\n",
       "      <td>[0.005893571288244606, -0.0998533897599655, 0....</td>\n",
       "      <td>[-0.12103361407151589, 0.041479411718543045, 0...</td>\n",
       "      <td>[-0.1269271853597605, 0.14133280147850855, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                    stemmed_premise  \\\n",
       "0      3690  [among, the, sight, in, bezier, ar, the, ancie...   \n",
       "1      8344  [in, april, the, sultan, armi, mass, outsid, t...   \n",
       "2      5426  [when, the, two, nation, divid, it, up, franc,...   \n",
       "3      6436  [over, most, of, the, and, the, us, wa, abl, t...   \n",
       "4      1794  [in, my, crossfir, dai, wa, patron, even, by, ...   \n",
       "...     ...                                                ...   \n",
       "5491   1596  [and, and, so, you, know, like, everi, other, ...   \n",
       "5492   9183  [thi, make, it, incumb, on, the, govern, to, c...   \n",
       "5493   7027  [the, govern, statut, provid, that, committe, ...   \n",
       "5494    415    [there, ar, two, challeng, to, these, top, dog]   \n",
       "5495   9119  [on, naxo, you, can, walk, through, the, prett...   \n",
       "\n",
       "                                           stemmed_hypo  \\\n",
       "0     [there, is, noth, interest, about, bezier, bec...   \n",
       "1     [there, were, ten, time, as, mani, of, the, su...   \n",
       "2          [franc, end, up, not, get, the, salt, pound]   \n",
       "3     [the, us, could, invest, more, than, it, save,...   \n",
       "4     [dure, crossfir, even, sam, donaldson, patron,...   \n",
       "...                                                 ...   \n",
       "5491  [ticket, to, see, chima, para, diso, cost, monei]   \n",
       "5492  [there, is, no, need, for, the, govern, to, cr...   \n",
       "5493  [no, on, can, recommend, an, individu, to, the...   \n",
       "5494  [these, top, dog, face, two, tough, financi, c...   \n",
       "5495  [instead, of, walk, through, the, villag, of, ...   \n",
       "\n",
       "                                               vector_p  \\\n",
       "0     [0.0042721433298928396, -0.03233098472202463, ...   \n",
       "1     [-0.0676229025957582, -0.05701556266285479, 0....   \n",
       "2     [0.062390736575859286, -0.016578831140779786, ...   \n",
       "3     [0.06283004954457283, 0.02205984862521291, -0....   \n",
       "4     [0.11890019625425338, -0.02407050523906946, 0....   \n",
       "...                                                 ...   \n",
       "5491  [0.01040780481191412, -0.04228932386444461, -0...   \n",
       "5492  [0.04760168118001376, 0.06779161074923144, -0....   \n",
       "5493  [-0.05681318695270369, -0.09089089054526445, -...   \n",
       "5494  [-0.014065003953874111, 0.06840262585319579, -...   \n",
       "5495  [0.005893571288244606, -0.0998533897599655, 0....   \n",
       "\n",
       "                                               vector_h  \\\n",
       "0     [0.0435828339929382, 0.23425666491190592, -0.0...   \n",
       "1     [-0.02018942977883853, 0.038256624015048146, -...   \n",
       "2     [-0.023860372428316623, 0.05636150110512972, 0...   \n",
       "3     [0.03486745432019234, 0.015463362024589018, -0...   \n",
       "4     [-0.05265214440545866, -0.13422715025288717, -...   \n",
       "...                                                 ...   \n",
       "5491  [0.04032112471759319, -0.16984214953845367, 0....   \n",
       "5492  [-0.07817578880737225, 0.01920080160101255, -0...   \n",
       "5493  [-0.012223369817630473, -0.08494136313145811, ...   \n",
       "5494  [-0.07580050267279148, 0.35522675164975226, 0....   \n",
       "5495  [-0.12103361407151589, 0.041479411718543045, 0...   \n",
       "\n",
       "                                             difference  \n",
       "0     [0.03931069066304536, 0.26658764963393056, -0....  \n",
       "1     [0.047433472816919675, 0.09527218667790294, -0...  \n",
       "2     [-0.0862511090041759, 0.07294033224590951, 0.1...  \n",
       "3     [-0.027962595224380493, -0.006596486600623891,...  \n",
       "4     [-0.17155234065971203, -0.11015664501381771, -...  \n",
       "...                                                 ...  \n",
       "5491  [0.029913319905679074, -0.12755282567400905, 0...  \n",
       "5492  [-0.125777469987386, -0.048590809148218894, -0...  \n",
       "5493  [0.04458981713507321, 0.0059495274138063325, -...  \n",
       "5494  [-0.06173549871891737, 0.2868241257965565, 0.1...  \n",
       "5495  [-0.1269271853597605, 0.14133280147850855, 0.0...  \n",
       "\n",
       "[5496 rows x 6 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train[['difference']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = []\n",
    "# Converting the tokens into the format that the model requires\n",
    "for index, row in X_train_final.iterrows():\n",
    "    # Converting the tokens into the format that the model requires\n",
    "    train_features.append(row['difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5496"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.93106907e-02,  2.66587650e-01, -7.82855777e-02,  5.56384352e-02,\n",
       "       -2.13959938e-02,  7.43678132e-02, -2.05250437e-01,  3.04051035e-02,\n",
       "       -1.47274896e-02, -3.61418145e-01,  1.37039490e-01,  7.50855163e-03,\n",
       "        3.65080640e-02,  1.03687812e-02, -3.53117515e-02,  8.02945666e-02,\n",
       "        9.45577769e-02,  3.97322274e-02,  1.74652939e-02, -4.56277766e-02,\n",
       "        1.32031097e-02, -1.40510167e-01,  2.12676325e-02,  2.31862796e-01,\n",
       "        3.63970387e-03, -8.37174732e-04, -1.18871357e-01,  1.31963645e-02,\n",
       "        4.51720246e-02,  8.51440842e-02, -2.38067959e-01,  1.06340985e-01,\n",
       "        7.57768676e-02,  8.32202706e-02, -1.57757260e-01, -1.22112440e-01,\n",
       "       -1.53921374e-01, -1.39812066e-01,  9.01413891e-02,  1.34829603e-01,\n",
       "       -1.77534333e-03,  1.43088047e-01, -7.86728644e-02, -9.85791904e-02,\n",
       "       -1.72731430e-01, -5.73730169e-02, -1.17051365e-01,  3.22344946e-02,\n",
       "        1.26556057e-01, -1.54348696e-01, -2.91151277e-01,  2.02672176e-02,\n",
       "       -2.93926239e-01, -2.18221971e-04, -4.97268261e-02, -2.02675102e-01,\n",
       "        5.04964031e-03, -1.89907011e-01,  1.22091549e-01,  1.17667411e-01,\n",
       "       -1.41258059e-02, -2.04460555e-01, -1.40606347e-02,  3.04026445e-01,\n",
       "       -3.72469108e-03, -1.87971022e-02, -5.68077523e-02, -2.15058456e-01,\n",
       "        6.15652388e-02, -2.12614107e-02, -4.07487752e-02,  6.76812763e-02,\n",
       "       -5.80234438e-02,  7.72611780e-02,  6.09534263e-02,  2.73411837e-02,\n",
       "        3.53888777e-01,  5.07621900e-02,  6.76874541e-02, -1.23553991e-01,\n",
       "       -1.99491256e-01, -1.96442657e-01, -1.49424097e-01, -4.04707467e-02,\n",
       "        8.48281308e-02, -5.56286085e-02,  1.13281490e-01, -1.12929070e-01,\n",
       "       -6.09677847e-02,  2.76215224e-02,  1.13771425e-01, -1.16963829e-01,\n",
       "       -1.39398183e-01,  1.85015591e-01, -2.83343046e-02,  8.02590749e-03,\n",
       "        4.45343188e-01,  4.67685795e-02,  1.46851501e-01,  3.04209564e-02,\n",
       "       -1.15997724e-01,  8.71579340e-02, -1.18605432e-01,  8.02455625e-02,\n",
       "       -1.78162636e-02, -1.39398920e-01,  1.29054603e-01, -1.72089017e-02,\n",
       "        2.58148842e-01, -1.51327007e-01, -8.90733645e-02, -1.93279669e-01,\n",
       "       -9.92888226e-02,  4.75738718e-02, -2.88883627e-01,  2.54154113e-01,\n",
       "        8.15942933e-02, -1.40377564e-01, -2.12062373e-02, -2.88544139e-02,\n",
       "        1.17606598e-01, -2.04020662e-01, -3.77985479e-02, -1.25044974e-01,\n",
       "        1.04823782e-01, -4.72872762e-02, -1.73119610e-01,  1.41138221e-01,\n",
       "        5.27852277e-02,  1.71242115e-01, -1.03420263e-01, -7.05769674e-02,\n",
       "        2.34175650e-02, -1.31063893e-01, -8.87493408e-02, -1.61738980e-01,\n",
       "       -1.00199745e-04, -1.77739153e-02, -2.44226080e-01,  1.56823448e-01,\n",
       "        1.11281324e-02,  1.74497761e-02,  1.17126204e-01, -5.81699595e-02,\n",
       "        9.18197773e-02, -2.80761754e-01,  1.71726580e-01, -2.38901913e-01,\n",
       "       -2.38182838e-01, -1.90644580e-01, -2.61999640e-02, -6.25691289e-02,\n",
       "        1.10712460e-01, -1.27691814e-01,  1.96027110e-01, -3.25509156e-02,\n",
       "       -4.58731699e-02, -1.49547642e-01, -1.66064287e-01,  4.21489328e-02,\n",
       "        9.28409634e-03, -1.60218105e-02, -1.19370342e-01, -9.32717102e-02,\n",
       "        4.31960952e-02, -6.45633141e-02, -1.99431200e-02,  1.91668783e-02,\n",
       "        1.74315222e-02, -2.75885693e-01, -8.88437403e-02, -7.58022629e-02,\n",
       "       -1.22067241e-01, -2.04010234e-01, -2.04998942e-01,  1.64299355e-02,\n",
       "       -9.16533673e-02, -6.25494818e-02,  1.11691916e-02, -5.88189744e-02,\n",
       "        1.18299812e-01,  2.40437889e-01,  1.18538552e-02, -8.23877971e-02,\n",
       "       -1.21603123e-01, -1.13538802e-01,  6.85342686e-02, -2.47675582e-02,\n",
       "        5.42687140e-02,  3.42577063e-02, -7.01402273e-02,  7.59067233e-02,\n",
       "        2.09581203e-01, -2.43693220e-01, -1.32984317e-02,  1.13175660e-01,\n",
       "       -4.47072511e-03, -1.34074568e-01,  8.72315393e-02, -1.45698892e-01,\n",
       "        3.31906911e-01, -1.34214078e-02, -1.74555724e-01,  7.26371659e-02,\n",
       "        1.71625413e-02, -1.01728317e-03, -2.02555749e-01, -1.85729599e-01,\n",
       "        1.55088300e-01,  2.79245405e-02,  9.16561072e-03, -3.73574587e-02,\n",
       "        9.55082571e-02, -2.38351432e-01,  9.35038394e-02,  3.35789071e-02,\n",
       "        9.88334848e-03, -2.26866376e-02, -2.28462651e-02, -1.70030440e-01,\n",
       "        2.36805087e-03, -1.10011771e-01, -1.44416877e-01, -9.19672234e-02,\n",
       "        6.09601481e-01,  1.51162188e-01,  6.16078943e-02,  7.29992999e-02,\n",
       "        9.12955641e-03, -9.22259506e-03, -1.26913712e-01,  2.21876705e-01,\n",
       "       -1.18888724e-01,  1.60695000e-01, -7.96907939e-02,  1.78000289e-02,\n",
       "        1.67085545e-01, -1.19902117e-01,  1.46119667e-01,  4.59112484e-02,\n",
       "        1.11493246e-01, -1.58902500e-01, -2.17287485e-01, -1.07015070e-01,\n",
       "       -2.58429698e-02, -1.04789021e-01, -1.30328784e-01,  1.86005352e-01,\n",
       "        2.24176928e-01, -2.40443050e-02,  1.07979774e-01, -9.01541370e-02,\n",
       "        6.73684629e-02,  1.33992755e-01,  1.27045505e-01, -7.03222054e-02,\n",
       "        2.71390161e-01, -1.87630773e-01, -1.02571914e-01, -6.58346115e-02,\n",
       "       -1.68052087e-01, -2.62159164e-02, -2.97539374e-02, -1.37970488e-01,\n",
       "       -7.98946656e-02,  1.27577776e-01, -6.86263634e-02,  1.20950627e-01,\n",
       "        2.66944936e-01, -3.09335633e-02,  1.90069453e-01, -3.45520619e-02,\n",
       "        3.78703882e-02,  3.59487911e-02, -1.77934709e-01, -1.37590926e-01,\n",
       "        3.80021854e-02, -4.12819700e-02,  9.51271156e-02,  8.13811006e-02,\n",
       "        1.77760221e-01, -1.92971851e-01,  2.37694051e-02,  8.39132585e-02,\n",
       "       -8.24760189e-02,  9.97064002e-02,  2.74243025e-02,  1.44735593e-01,\n",
       "        6.93427573e-02,  4.07548727e-03,  2.73920823e-02,  5.56235333e-02,\n",
       "       -7.42405181e-02,  2.23704365e-01,  6.00541132e-02, -5.23026247e-02,\n",
       "        2.08851956e-02,  1.67801542e-01,  2.63096012e-01, -1.10533772e-02])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=4000)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Let us try linear SVC model\n",
    "SVC = LinearSVC(max_iter=4000)\n",
    "SVC.fit(train_features, Y_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=200)\n",
    "clf2.fit(train_features, Y_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-149-f69f7a950f12>:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  array_vector_p = np.array(array_vector_p).astype(np.float)\n",
      "<ipython-input-149-f69f7a950f12>:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  array_vector_h = np.array(array_vector_h).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_features_p = []\n",
    "test_features_h = []\n",
    "test_features_difference = []\n",
    "for index, row in X_test.iterrows():\n",
    "    array_vector_p = []\n",
    "    for token in row['stemmed_premise']:\n",
    "        word_vector = get_word_vector(token)\n",
    "        array_vector_p.append(word_vector)\n",
    "        \n",
    "    array_vector_h = []\n",
    "    for token in row['stemmed_hypo']:\n",
    "        word_vector = get_word_vector(token)\n",
    "        array_vector_h.append(word_vector)\n",
    "        \n",
    "    array_vector_p = np.array(array_vector_p).astype(np.float)\n",
    "    array_vector_h = np.array(array_vector_h).astype(np.float)\n",
    "    \n",
    "#     if(index == 0):\n",
    "#         print(array_vector_p)\n",
    "    model_vector_p = (np.mean(array_vector_p, axis=0)).tolist()\n",
    "    model_vector_h = (np.mean(array_vector_h, axis=0)).tolist()\n",
    "    \n",
    "    if type(model_vector_p) is list:\n",
    "        test_features_p.append(model_vector_p)\n",
    "    else:\n",
    "        test_features_p.append(np.array([0 for i in range(300)]))\n",
    "        \n",
    "    if type(model_vector_h) is list:\n",
    "        test_features_h.append(model_vector_h)\n",
    "    else:\n",
    "        test_features_h.append(np.array([0 for i in range(300)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_features_h)):\n",
    "    difference = np.array(test_features_h[i]) - np.array(test_features_p[i])\n",
    "    test_features_difference.append(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.53      0.51       489\n",
      "           1       0.42      0.37      0.39       444\n",
      "           2       0.46      0.47      0.46       441\n",
      "\n",
      "    accuracy                           0.46      1374\n",
      "   macro avg       0.46      0.46      0.45      1374\n",
      "weighted avg       0.46      0.46      0.46      1374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions_word2vec = clf2.predict(test_features_difference)\n",
    "print(classification_report(Y_test['label'],test_predictions_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.46      0.45       489\n",
      "           1       0.39      0.34      0.36       444\n",
      "           2       0.48      0.51      0.50       441\n",
      "\n",
      "    accuracy                           0.44      1374\n",
      "   macro avg       0.43      0.44      0.43      1374\n",
      "weighted avg       0.43      0.44      0.43      1374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions_word2vec_svc = SVC.predict(test_features_difference)\n",
    "print(classification_report(Y_test['label'],test_predictions_word2vec_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
